{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19b88bb-44fb-4484-82b1-c4a248ca4886",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING\n",
    "##### Required files in the same directory \n",
    "\n",
    "# trumpfinal.csv, kamalafinal.csv, data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0179e0fb-207f-4199-8aae-6a2db599b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/16 23:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of data.csv:\n",
      "+-------+--------------------+--------------------+-----+--------------+-------------------+---------+--------------------+-----------+--------------------+\n",
      "|     id|               title|            selftext|score|comments_count|        created_utc|subreddit|                 url|      topic|            comments|\n",
      "+-------+--------------------+--------------------+-----+--------------+-------------------+---------+--------------------+-----------+--------------------+\n",
      "| bfht1b|What I think abou...|                NULL| 3184|           169|2019-04-20 22:16:30| Firearms|https://i.redd.it...|Gun Control|\"['Australia has ...|\n",
      "| o5lhvu|CNN accidentally ...|                NULL| 3024|           408|2021-06-22 12:02:59| Firearms|https://i.imgur.c...|Gun Control|\"[\"\"It's almost l...|\n",
      "| mbviya|     Gun control now|                NULL| 2991|           132|2021-03-24 02:38:34| Firearms|https://i.redd.it...|Gun Control|\"['You had me in ...|\n",
      "| j2kopd|\"The only kind of...|                NULL| 2975|           259|2020-09-30 11:53:44| Firearms|https://v.redd.it...|Gun Control|\"['When I\\'m argu...|\n",
      "|1fw497a|Shall not be INFR...|Every gun law is ...| 2898|           146|2024-10-04 17:05:54| Firearms|https://i.redd.it...|Gun Control|\"['That was an ep...|\n",
      "+-------+--------------------+--------------------+-----+--------------+-------------------+---------+--------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Head of trump2024.csv:\n",
      "+--------------------+--------------------+--------------------+---------+-------------+------------------+---------------+--------------------+----------------+----------------+--------------+--------------------+-------------+----+----+----+-------+---------+-----+----------+------------+\n",
      "|          created_at|            tweet_id|               tweet|    likes|retweet_count|            source|        user_id|           user_name|user_screen_name|user_description|user_join_date|user_followers_count|user_location| lat|long|city|country|continent|state|state_code|collected_at|\n",
      "+--------------------+--------------------+--------------------+---------+-------------+------------------+---------------+--------------------+----------------+----------------+--------------+--------------------+-------------+----+----+----+-------+---------+-----+----------+------------+\n",
      "| 2024-10-15 00:00:01|1.316529221557252...|#Elecciones2020 |...|     NULL|         NULL|              NULL|           NULL|                NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|NULL|   NULL|     NULL| NULL|      NULL|        NULL|\n",
      "|                   â €|                NULL|                NULL|     NULL|         NULL|              NULL|           NULL|                NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|NULL|   NULL|     NULL| NULL|      NULL|        NULL|\n",
      "|ðŸŒhttps://t.co/qh...|                NULL|                NULL|     NULL|         NULL|              NULL|           NULL|                NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|NULL|   NULL|     NULL| NULL|      NULL|        NULL|\n",
      "|                   _|                NULL|                NULL|     NULL|         NULL|              NULL|           NULL|                NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|NULL|   NULL|     NULL| NULL|      NULL|        NULL|\n",
      "|#ElSolLatino #yob...|                 0.0|                 0.0|TweetDeck|  360666534.0|El Sol Latino News|elsollatinonews|ðŸŒ Noticias de in...|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|NULL|   NULL|     NULL| NULL|      NULL|        NULL|\n",
      "+--------------------+--------------------+--------------------+---------+-------------+------------------+---------------+--------------------+----------------+----------------+--------------+--------------------+-------------+----+----+----+-------+---------+-----+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema of data.csv:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- selftext: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- comments_count: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n",
      "Schema of trump2024.csv:\n",
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- tweet: string (nullable = true)\n",
      " |-- likes: string (nullable = true)\n",
      " |-- retweet_count: string (nullable = true)\n",
      " |-- source: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_name: string (nullable = true)\n",
      " |-- user_screen_name: string (nullable = true)\n",
      " |-- user_description: string (nullable = true)\n",
      " |-- user_join_date: string (nullable = true)\n",
      " |-- user_followers_count: string (nullable = true)\n",
      " |-- user_location: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- collected_at: string (nullable = true)\n",
      "\n",
      "Summary of data.csv:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 23:53:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                  id|               title|            selftext|               score|      comments_count|       created_utc|           subreddit|                 url|               topic|            comments|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|               41298|               25822|               13653|               15628|               13768|             12540|               11702|               11145|               10578|               10237|\n",
      "|   mean|            Infinity|   960.6665745856354|   622.5316129032258|  2114.2751404151404|  303.96983135540285| 884.9088471849866|  473.13170731707316|  219.19130434782608|   20496.25396825397|  1028.1666666666667|\n",
      "| stddev|                 NaN|   7731.804385721151|   4934.297719245274|   8698.992110341685|  2025.8398594292958|6121.8880044460675|  3067.8934487625716|   880.5429634322439|  150813.79117511728|  2847.2096089805655|\n",
      "|    min|  \\t\\t\\t\\t\\t\\t\\tLove|\\t   A\\t   Divide...|\\t   Confiscate\\t   |           \\t   walk|\\t   or\\t   tramp...|        \\t   crush|\\t   or\\t   injur...|\\t   it\\t   refer...|\\t   Liberty\\t   ...|\\t   regardless\\t...|\n",
      "|    max|ðŸ”—Â [Read the full...|                ðŸ˜‚ðŸ˜‚|ðŸ¤žHoping they get...|â€* which are only...|â€well that sounds...|    â€ â€œimperialism|           â€ â€œLatinx|            â€ â€œLGBTQ|â€ â€œthe more capab...|â€ which swiftly a...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "Summary of trump2024.csv:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|summary|          created_at|            tweet_id|               tweet|               likes|       retweet_count|              source|             user_id|           user_name|    user_screen_name|    user_description|      user_join_date|user_followers_count|       user_location|                 lat|                long|                city|             country|           continent|               state|          state_code|        collected_at|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  count|             1747879|             1221054|             1180870|              834348|              794754|              788969|              760237|              757895|              708484|              643058|              572291|              486633|              365463|              238160|              281705|              199255|              251448|              245655|              139249|              128379|                4377|\n",
      "|   mean|1.224309962858195...|9.400649767757761...|   5254.104678693215|            Infinity|1.942787720665089...|7.512172087799817...|4.028029043328488...|                 NaN|2.280417294105285...| 6.69561585108685E97|1.361512639248086...|2.197804651760499...|2.411656053500529...|1.213707604224089...|3.770118091947003E13|1.149579882297809E15|1.070522045001792...|1.447744718925114...|6.178839940803665E15|1.114461585091857...|1.206072332147923...|\n",
      "| stddev|1.471201952307698...|5.989218276453176...|  113467.78004621372|                 NaN|4.280403739763070...|2.790384090824367...|5.424671267595402...|                 NaN|4.468709008571187...|3.128952342251605...|1.214776961234901...|1.594133747436310...|1.607654757227150...|1.196569102628094...|6.520597283027535E15|3.461493684569666E16|3.582057478310178E16|4.369695508544792E16|8.891424941226916...|1.152494862141577...|1.229957271285232...|\n",
      "|    min|    \\bEduHK student\"|                    |                    |                    |                    |                    |                    |                    |                    |   Edith Massola ...|                    |                    |                  \\t|  #Liverpool  and...|                    | \"\"media\"\" specia...|           #CSKvsRR |              HÃ¼pper|              #Trump| #Trump2020 #GodG...|       BatÄ± Virginia|\n",
      "|    max|   ðŸª° Joe Biden +200|                 ðŸ›³ï¸|ðŸ§µðŸ§¶Portadas del ...|ðŸ§Ÿâ€â™‚ï¸ The Zombie ...|ðŸ‘‰DM for pomotionsðŸ¤”|                  ðŸª”|ðŸ§™Roanna Carleton...|ðŸª´ðŸŒŒðŸ‡ºðŸ‡¸ Sheri Ho...|ðŸ§¶Complejo âš™ï¸ Com...|   ðŸªðŸ¦¸ðŸ»â€â™‚ï¸â˜„ï¸ðŸ³ï¸â€ðŸŒˆ|                  ðŸª|ðŸ¥‡ La radio piÃ¹ a...|               ðŸªðŸŒâœ¨|ðŸš«ðŸ˜¡PORN AND SCAM...|ðŸŸ¢ ONLINE | OCTOB...|ðŸŽµ#GobiernoTralarÃ¡ðŸŽµ|             ðŸ“ Ulm |ðŸ‡·ðŸ‡ºRACKETEER PRO...|    ÅÃ³dÅº Voivodeship|  Ãngulo Informativo|Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "Number of rows in data.csv: 41299\n",
      "Number of rows in trumpfinal.csv: 1748108\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('CSVHeadExample').getOrCreate()\n",
    "data_df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "trump2024_df = spark.read.csv('trumpfinal.csv', header=True, inferSchema=True)\n",
    "print(\"Head of data.csv:\")\n",
    "data_df.show(5)\n",
    "\n",
    "print(\"Head of trump2024.csv:\")\n",
    "trump2024_df.show(5)\n",
    "\n",
    "print(\"Schema of data.csv:\")\n",
    "data_df.printSchema()\n",
    "\n",
    "print(\"Schema of trump2024.csv:\")\n",
    "trump2024_df.printSchema()\n",
    "\n",
    "print(\"Summary of data.csv:\")\n",
    "data_df.describe().show()\n",
    "\n",
    "print(\"Summary of trump2024.csv:\")\n",
    "trump2024_df.describe().show()\n",
    "\n",
    "data_row_count = data_df.count()\n",
    "trump2024_row_count = trump2024_df.count()\n",
    "\n",
    "print(f\"Number of rows in data.csv: {data_row_count}\")\n",
    "print(f\"Number of rows in trumpfinal.csv: {trump2024_row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5062cadb-c18b-4608-8bb5-8916c3ad0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/16 23:54:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----+-----+-------------+------+-------+---------+----------------+----------------+--------------+--------------------+-------------+----+----+-------+-------+---------+-------+----------+------------+\n",
      "|         created_at|tweet_id|tweet|likes|retweet_count|source|user_id|user_name|user_screen_name|user_description|user_join_date|user_followers_count|user_location| lat|long|   city|country|continent|  state|state_code|collected_at|\n",
      "+-------------------+--------+-----+-----+-------------+------+-------+---------+----------------+----------------+--------------+--------------------+-------------+----+----+-------+-------+---------+-------+----------+------------+\n",
      "|2019-04-20 22:16:30|  bfht1b| NULL| 3184|          169|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2021-06-22 12:02:59|  o5lhvu| NULL| 3024|          408|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2021-03-24 02:38:34|  mbviya| NULL| 2991|          132|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2020-09-30 11:53:44|  j2kopd| NULL| 2975|          259|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2024-10-04 17:05:54| 1fw497a| NULL| 2898|          146|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "+-------------------+--------+-----+-----+-------------+------+-------+---------+----------------+----------------+--------------+--------------------+-------------+----+----+-------+-------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, unix_timestamp, coalesce, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditToTwitterConversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "data_df = spark.read.option(\"header\", \"true\").csv(\"data.csv\")\n",
    "kamala2024_df = spark.read.option(\"header\", \"true\").csv(\"kamalafinal.csv\")\n",
    "data_df = data_df.withColumn(\"created_at\", unix_timestamp(col(\"created_utc\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "data_df = data_df.withColumnRenamed(\"id\", \"tweet_id\")\n",
    "data_df = data_df.withColumn(\n",
    "    \"tweet\", \n",
    "    coalesce(col(\"title\"), lit(\"\")) + \" \" + coalesce(col(\"selftext\"), lit(\"\"))\n",
    ")\n",
    "\n",
    "data_df = data_df.withColumn(\"likes\", col(\"score\"))\n",
    "data_df = data_df.withColumn(\"retweet_count\", col(\"comments_count\"))\n",
    "data_df = data_df.withColumn(\"source\", lit(\"Reddit\"))\n",
    "\n",
    "\n",
    "data_df = data_df.withColumn(\"user_id\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_name\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_screen_name\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_description\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_join_date\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_followers_count\", lit(None).cast(\"integer\"))\n",
    "data_df = data_df.withColumn(\"user_location\", lit(None).cast(\"string\"))\n",
    "\n",
    "\n",
    "data_df = data_df.withColumn(\"lat\", lit(None).cast(\"double\"))\n",
    "data_df = data_df.withColumn(\"long\", lit(None).cast(\"double\"))\n",
    "data_df = data_df.withColumn(\"city\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"country\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"continent\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"state\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"state_code\", lit(\"Unknown\"))\n",
    "\n",
    "data_df = data_df.withColumn(\"collected_at\", unix_timestamp(lit(\"now\")).cast(\"timestamp\"))\n",
    "\n",
    "final_df = data_df.select(\n",
    "    \"created_at\",\n",
    "    \"tweet_id\",\n",
    "    \"tweet\",\n",
    "    \"likes\",\n",
    "    \"retweet_count\",\n",
    "    \"source\",\n",
    "    \"user_id\",\n",
    "    \"user_name\",\n",
    "    \"user_screen_name\",\n",
    "    \"user_description\",\n",
    "    \"user_join_date\",\n",
    "    \"user_followers_count\",\n",
    "    \"user_location\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"city\",\n",
    "    \"country\",\n",
    "    \"continent\",\n",
    "    \"state\",\n",
    "    \"state_code\",\n",
    "    \"collected_at\"\n",
    ")\n",
    "\n",
    "final_df.show(5)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc2821f-0288-4514-b282-e0ec824546f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----+-----+-------------+------+-------+---------+----------------+----------------+--------------+--------------------+-------------+----+----+-------+-------+---------+-------+----------+------------+\n",
      "|         created_at|tweet_id|tweet|likes|retweet_count|source|user_id|user_name|user_screen_name|user_description|user_join_date|user_followers_count|user_location| lat|long|   city|country|continent|  state|state_code|collected_at|\n",
      "+-------------------+--------+-----+-----+-------------+------+-------+---------+----------------+----------------+--------------+--------------------+-------------+----+----+-------+-------+---------+-------+----------+------------+\n",
      "|2019-04-20 22:16:30|  bfht1b| NULL| 3184|          169|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2021-06-22 12:02:59|  o5lhvu| NULL| 3024|          408|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2021-03-24 02:38:34|  mbviya| NULL| 2991|          132|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2020-09-30 11:53:44|  j2kopd| NULL| 2975|          259|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "|2024-10-04 17:05:54| 1fw497a| NULL| 2898|          146|Reddit|   NULL|     NULL|            NULL|            NULL|          NULL|                NULL|         NULL|NULL|NULL|Unknown|Unknown|  Unknown|Unknown|   Unknown|        NULL|\n",
      "+-------------------+--------+-----+-----+-------------+------+-------+---------+----------------+----------------+--------------+--------------------+-------------+----+----+-------+-------+---------+-------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, unix_timestamp, coalesce, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditToTwitterConversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "data_df = spark.read.option(\"header\", \"true\").csv(\"data.csv\")\n",
    "trump2024_df = spark.read.option(\"header\", \"true\").csv(\"trumpfinal.csv\")\n",
    "data_df = data_df.withColumn(\"created_at\", unix_timestamp(col(\"created_utc\"), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "data_df = data_df.withColumnRenamed(\"id\", \"tweet_id\")\n",
    "data_df = data_df.withColumn(\n",
    "    \"tweet\", \n",
    "    coalesce(col(\"title\"), lit(\"\")) + \" \" + coalesce(col(\"selftext\"), lit(\"\"))\n",
    ")\n",
    "\n",
    "data_df = data_df.withColumn(\"likes\", col(\"score\"))\n",
    "data_df = data_df.withColumn(\"retweet_count\", col(\"comments_count\"))\n",
    "data_df = data_df.withColumn(\"source\", lit(\"Reddit\"))\n",
    "\n",
    "\n",
    "data_df = data_df.withColumn(\"user_id\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_name\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_screen_name\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_description\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_join_date\", lit(None).cast(\"string\"))\n",
    "data_df = data_df.withColumn(\"user_followers_count\", lit(None).cast(\"integer\"))\n",
    "data_df = data_df.withColumn(\"user_location\", lit(None).cast(\"string\"))\n",
    "\n",
    "\n",
    "data_df = data_df.withColumn(\"lat\", lit(None).cast(\"double\"))\n",
    "data_df = data_df.withColumn(\"long\", lit(None).cast(\"double\"))\n",
    "data_df = data_df.withColumn(\"city\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"country\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"continent\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"state\", lit(\"Unknown\"))\n",
    "data_df = data_df.withColumn(\"state_code\", lit(\"Unknown\"))\n",
    "\n",
    "data_df = data_df.withColumn(\"collected_at\", unix_timestamp(lit(\"now\")).cast(\"timestamp\"))\n",
    "\n",
    "final_df = data_df.select(\n",
    "    \"created_at\",\n",
    "    \"tweet_id\",\n",
    "    \"tweet\",\n",
    "    \"likes\",\n",
    "    \"retweet_count\",\n",
    "    \"source\",\n",
    "    \"user_id\",\n",
    "    \"user_name\",\n",
    "    \"user_screen_name\",\n",
    "    \"user_description\",\n",
    "    \"user_join_date\",\n",
    "    \"user_followers_count\",\n",
    "    \"user_location\",\n",
    "    \"lat\",\n",
    "    \"long\",\n",
    "    \"city\",\n",
    "    \"country\",\n",
    "    \"continent\",\n",
    "    \"state\",\n",
    "    \"state_code\",\n",
    "    \"collected_at\"\n",
    ")\n",
    "\n",
    "final_df.show(5)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183e5e67-177b-441b-b086-b0ea2acbc003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the cleaned data: 5228\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName('CSVHeadExample').getOrCreate()\n",
    "\n",
    "data_df = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
    "trump2024_df = spark.read.csv('trumpfinal.csv', header=True, inferSchema=True)\n",
    "\n",
    "data_df_cleaned = data_df.withColumn('score', F.col('score').cast('double'))\n",
    "data_df_cleaned = data_df_cleaned.withColumn('comments_count', F.col('comments_count').cast('double'))\n",
    "\n",
    "data_df_cleaned = data_df_cleaned.fillna({'title': 'Unknown', 'selftext': 'Unknown'})\n",
    "\n",
    "q1 = data_df_cleaned.approxQuantile(\"score\", [0.25], 0.05)[0]\n",
    "q3 = data_df_cleaned.approxQuantile(\"score\", [0.75], 0.05)[0]\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "data_df_cleaned = data_df_cleaned.filter((data_df_cleaned.score >= lower_bound) & (data_df_cleaned.score <= upper_bound))\n",
    "\n",
    "q1_comments = data_df_cleaned.approxQuantile(\"comments_count\", [0.25], 0.05)[0]\n",
    "q3_comments = data_df_cleaned.approxQuantile(\"comments_count\", [0.75], 0.05)[0]\n",
    "iqr_comments = q3_comments - q1_comments\n",
    "lower_bound_comments = q1_comments - 1.5 * iqr_comments\n",
    "upper_bound_comments = q3_comments + 1.5 * iqr_comments\n",
    "data_df_cleaned = data_df_cleaned.filter((data_df_cleaned.comments_count >= lower_bound_comments) & (data_df_cleaned.comments_count <= upper_bound_comments))\n",
    "\n",
    "data_df_cleaned.write.csv('preprocessed.csv', header=True, mode =\"overwrite\")\n",
    "\n",
    "print(f\"Number of rows in the cleaned data: {data_df_cleaned.count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
